---
layout: single
title: "在浙大的半年"
date: 2026-01-15
categories: [随笔]
tags: [学习历程]
---

大家好，我是刘钦杰。这是我第一次搭建个人网站。借此机会，想发表一篇博客，与大家分享我这一路走来的历程。

我是一名浙大化工系的学生，但也是一名忠实的技术爱好者。我在小学时曾经学过围棋，获得了业余一段的证书，六年以来恰逢AlphaGo飞速发展，从最开始的AlphaGo到AlphaZero到AlphaZero 2.0...当时，我沉浸于观赏AlphaGo与人类顶尖棋手的对弈，也亲眼目睹了其颠覆一个又一个的围棋定式，深研算法的种子在我心中生根发芽。

高中时代，我学过全国数学联赛，对数论也有了一定了解。高中学校有计算机课，虽因计算机成绩不直接计入高考总分，课数不多，讲得也不深入，那却是我第一次系统接近编程语言。当时，我痴迷于计算机课，通过自学，基本了解了Python的全部基础语法知识。

高三那年，DeepSeek火爆出圈，大语言人工智能被广泛应用于教育、医疗、心理健康等领域。在这一过程中，我进一步体会到了算法的伟力。我坚信，模式易学然技术难求，要想真正实现用技术于社会，造福人民，技术当是根基。也是在那时，我立下了到浙江大学主修计算机科学的志向。

录取结果未达而努力先行。高三毕业后，我便投身于计算机编程语言的系统学习。我听说C语言接近底层，目标代码运行效率高，学习难度也因此比Python高，这激起了我极大的好胜心。我便从C语言入手学习，一个暑假，从稚嫩的猜数字小游戏，到井字棋，到推箱子小游戏到2048......从数组，到指针，到链表，到文件，我一直都在探索前行的路上。当年暑假，我跟随翁恺老师的Mooc，系统学完了学校“C程序设计基础与实验”的前身（C大和C小）。

翁恺老师用Acllib库给我们展示了往期学生的优秀毕业作品，那绚丽至极的渲染看得我目瞪口呆。我于是开始学习Acllib库的使用方法，从基础的画线到静态布局到动态更新，到键盘交互......基本理解了Acllib库的使用方法后，我便着手实现我的第一个项目：Tetris俄罗斯方块。虽有高中数学竞赛的数学基础，但矩阵运算毕竟尚不熟悉，我又自学了线性代数的理论知识，明白了线性变换的基本原理。

2025年8月，时机成熟，我将所学的所有模块化设计的思想、线性变换、游戏状态管理、键盘交互等知识应用于Tetris俄罗斯方块，实现了多功能、具有优良游戏体验的俄罗斯方块项目。

9月20日，在成功补选上浙江大学纪守领教授的“C语言程序设计基础与实验”后，我开始向教授请教进一步巩固C语言基础、掌握更高级算法技巧的路径。教授对我说：“做做大数加减乘除法吧。”加减乘除吗？当时年少轻狂，竟以为这不过是再简单不过的编程小游戏。可真正上手时才发现困难重重。

我不愿意直接使用malloc申请一块超大的内存。在我看来，我要做那便要尽量做好，精益求精。我希望我的程序能很好地处理碎片化内存。于是，我选用了链表储存。开始时，为了方便读取数字，我直接使用十进制链表进行处理，使用了最最简单的逐位乘法和长除法。那时我依旧自信——直到，我的算法出现了巨大的瓶颈。我发现我的算法计算六万位大数乘法居然需要花费139秒！这是我无法忍受的，我希望我的算法能在有限时间内完成至少百万级别的大数的运算、能够完成千万级别大数运算。六万位只是一个起点。

我终于意识到了，大数运算对于当时的我来说不是一个简单的项目，它极大考验我算法实现的能力。从改变进制这一起点，我先后试了十亿进制，万进制和亿进制，最后却惊异地发现，万进制的运算速率居然与亿进制不相上下！我于内存与效率中权衡着，最终选择了万进制。而我现在清楚，当时的选择是多么正确。

随后的9月到11月，我在平衡课内学业之余几乎全身心沉浸于代码的世界。我曾多次因此熬到深夜，可不知为何，我感到额外闲适。从理解分治思想，完成递归形式的一代卡拉楚巴算法，到精简算法流程、优化内存处理，实现了高稳定运行的卡拉楚巴递归乘法。万进制和卡拉楚巴的连招直接把六万位大数乘法降到了1.4秒。

（微笑）你知道吗？有趣的是，我刚开始调试卡拉楚巴的时候每几次递归都要打印一两个信息，结果我后来删除这些打印，速度直接从1.4秒降至0.7秒。至此，从139秒到0.7秒，实现跨越性飞跃。

随后，我再次攻坚牛顿迭代除法（其实我也尝试过其他算法，但最终还是选择了Newton）。牛顿迭代除法的本质就是先利用Newton迭代法求出除数的倒数的近似值，然后再用倒数与被除数做乘法。在这一过程中，乘法算法的效率几乎决定了除法能有多快。可倒数究竟是近似值，如何确保最终的除法结果精确到整数呢？我在此亦花费了一定时间，最后豁然开朗：Newton法求出来的结果与实际值相差几乎只会有1、2左右，仅需在算出结果后利用乘法剪出最终余数，即可轻易求知真实值与计算值之间的差异。

完成稳定运行的卡拉楚巴卷积乘法与牛顿迭代除法后，我仍旧不满足，便向傅里叶变换挺进。可傅里叶变换有个大问题，那就是浮点数误差。我想实现的是高精度的大整数运算，这一误差是我难以接受的。于是，我开始寻求有无消除误差的办法。

幸运地，高中浅薄的数论基础在此发挥了作用——数论变换，又被称为有限域上的傅里叶变换，完美消除了浮点数的误差。模数限制下的幂运算与单位根具有近乎一致的完美数学形式，以原根代替单位根，将能实现真正整数精度的乘法运算。我于是又花了很多时间调适研究，终于调试出了稳定的DFT单模数非迭代形式的数论变换。

可我马上就发觉不对劲：数论变换有一个bug，那就是所运算的大数的各个进制位的数字之和要小于所选取的模数。可C语言数据类型有长度限制，利用费马小定理预计算模逆运算的结果可以大大提升计算速度，可这种办法在超大模数上同样不适用。我们难以写出一个非常非常大的模数来支持我们甚至只是万位级别的大数的运算。经过测试，我当时所选取的模数最多支持40位数乘法运算。

破局方法在何方呢？CRT（中国剩余定理）为我指明了方向。通过计算大数在三个模数下数论变换的结果，再利用中国剩余定理进行合并，即可实现可逆的数论变换。于是我进一步探索尝试，实现了多模数DFT的雏形，这也是NNT数论变换的雏形......

这个项目仍在继续，但我心知道阻且长。NNT实现后我还需深入系统底层，实现汇编级别的优化，这对于当时的我来说是难以实现的。而且，对于汇编级的优化来说，万进制可能并非最优解，以2的幂次为进制数可能才是。于是，我暂时放弃了进一步攻克的想法，暂时转向了其他感兴趣的领域的学习和研究。

有少数人不理解我，问我：“你怎么会对枯燥的敲代码活动感兴趣呀？”我笑答：“我在做的从来不是简单的重复敲代码，而是，探索那我眼中的未知之境啊！”

10月中下旬，我开始学习SVM和人工神经网络等传统机器学习算法。当时我主要还是在Dev-C++环境下进行，但很快遇到了瓶颈。Dev-C++是轻量化的编译工具，难以支撑过于复杂的项目。于是，我在助教的建议下改用VScode工具，并重拾Python作为第二计算机语言。

跟随胡浩基老师的Mooc，我花了好几周的时间，废寝忘食地看，专心致志地研读每一个算法。从最早直接用C语言利用支持向量机通过月份和气候区分不同城市、判断井字棋是否胜利；到纯C语言利用简单全连接的人工神经网络进行简单井字棋AI的9分类问题训练；到最后好奇地将SVM和简单人工神经网络结合，写了联合决策。

利用单一支持向量机预测井字棋是否胜利的交叉验证准确率大概97%到98%左右。由于模型简单，我利用人工神经网络也写了一个交叉验证的算法，最终准确率在98%到99%之间。联合决策经过多次调整超参数，准确率直接达到99.8%（偶尔）到100%（大多数时候），近乎完美。

于是，我心满意足地攻克下一难关：深度学习。与大多数初学者一样，Mnist是我接触卷积网络的第一站。它基础，却又不凡，在实现它的过程中，我切切实实掌握了深度学习算法的一般流程。我首次接触了Pytorch的世界......从图像分割、数字识别到人像检测，我始终在学习着，思考着，迫切着。

胡浩基老师开课时曾介绍过五子棋AI，我久久难忘！终于，11月时，我学到了强化学习部分，遂着手攻克五子棋AI难关。从深度学习到DQN到AlphaGo风格网络，从net1到net2到net15，到trick_net（这里实际应该是策略网络，当时命名随意，如今也不便更改）、value_net，从v1到v2到v3。所经历调参之艰辛，模型预训练100epoch却训练1epoch电源就被拔了之糟心，诸君可参见我AlphaGomoku的README文档。

只记得那时常常盯着训练日志看到深夜，也常常一个参数调不对，整个晚上的训练结果化为乌有。我乐在其中，大抵是因为每一次失败都让我更清楚：机器学习乃数学与工程结合之精巧吧。所幸的是，最终所得模型与我已然水平相当，结合探索深度为100的简单MCTS算法后，甚至能与一些网页版的专家级别五子棋AI打成平手（就是那种谁先手谁赢那种）。MCTS浅陋之至，本人暂删除准备重写，其余函数在v3中都重写了，诸君感兴趣，亦可一览。

---
写于2026年1月15日

刘钦杰，很高兴认识你。

我的个人网站刚刚上线，这是我的第一篇博客。未来还会在这里分享更多技术思考与学习笔记。

感谢阅读，我们路上见。欢迎交流，一起进步。